---
title: "(1) Machine learning by open dynamical systems, for open dynamical systems"
collection: portfolio
---
Recurrent neural networks (RNNs) are a class of powerful open system models in machine learning that have also enjoyed synergies with disciplines such as signal processing, optimization, systems and control theory, neuroscience and network science. The explosion of real-time data (physical or not) and the promising potential of using dynamical systems (physically realizable or not) for computation and to learn the data are opening up a wide range of foundational and practical problems. However, the dearth of rigorous analysis limits the usefulness of RNNs in addressing scientific questions. Therefore, a deep understanding of the working mechanism of RNNs and related models is pivotal to shed light on the properties of large and adaptive architectures, and to facilitate systematic design of the next generation of networks.

On one hand, we are primarily exploring and working on various mathematical aspects of RNNs and related models in the context of deep learning theory. On the other hand, we are always looking to apply the models and theory to study complex dynamical systems arising in science and engineering. The theory and applications go hand in hand for us. For theoretical analysis, we use tools and techniques from  stochastic analysis, rough paths theory, statistical learning, among others. For applications, we are primarily inspired by ideas and insights from statistical mechanics and nonlinear science. <i>Check out the relevant research outputs [<font color = "blue">here</font>](https://shoelim.github.io/publications/).</i>

<small>*<i>We are also interested to learn more about RNN-based models for understanding biological systems (brain) as well as quantum counterparts of RNNs for advancing quantum computation and machine learning.</i></small>
<br>
<br>
