---
#layout: archive
title: "Machine Learning, Dynamical Systems, and All That"
permalink: /DSxML/
author_profile: true
redirect_from:
  - /DSxML
---

## Personal Notebooks
Personal perspective of modern ML: 

**Deep learning can be viewed as an optimal control problem (solved using gradient descent based algorithms on top of some randomization tricks for a chosen objective) for randomly initialized dynamical systems (deep architectures) interacting with a noisy environment (large amount of possibly poor-quality data)**.
<br>


## Key Papers
### Modern ML x Landmarks:
- [Learning internal representations by error-propagation](https://apps.dtic.mil/sti/pdfs/ADA164453.pdf) (1986)
- [A theoretical framework for back-propagation](http://yann.lecun.com/exdb/publis/pdf/lecun-88.pdf) (1988)
- [Gradient-based learning applied to document recognition](https://ieeexplore.ieee.org/abstract/document/726791) (1998)
- [Learning Deep Architectures for AI](https://books.google.se/books?hl=en&lr=&id=cq5ewg7FniMC&oi=fnd&pg=PA1&dq=info:pYyS8T9g_kkJ:scholar.google.com&ots=Kpi8QTmpIy&sig=XfG1389bgdNINpRjGy67OReL9_c&redir_esc=y#v=onepage&q&f=false) (2009)
- [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a) (2010)
- [ImageNet classification with deep convolutional neural networks](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) (2012)
- [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199) (2013)
- [Explaining and harnessing adversarial examples](https://arxiv.org/abs/1412.6572) (2014)
- [Neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473) (2014)
- [Adam: A method for stochastic optimization](https://arxiv.org/abs/1412.6980) (2014)
- [Dropout: A simple way to prevent neural networks from
overfitting](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer,) (2014)
- [Deep residual learning for image recognition](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) (2016)
- [Generative Adversarial Nets](https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html) (2014)
- [Batch normalization: accelerating deep network training by reducing internal covariate shift](https://arxiv.org/abs/1502.03167) (2015)
- [Attention is all you need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) (2017)
- [On large-batch training for deep learning: generalization gap and sharp minima](https://arxiv.org/abs/1609.04836) (2017)
- [Super-convergence: very fast training of neural networks using large learning rates](https://www.spiedigitallibrary.org/conference-proceedings-of-spie/11006/1100612/Super-convergence--very-fast-training-of-neural-networks-using/10.1117/12.2520589.full?SSO=1) (2019)
- [Understanding deep learning requires rethinking generalization](https://dl.acm.org/doi/abs/10.1145/3446776) (2017, 2021)

### Using DS to Improve ML:
- [Randomized iterative methods for linear systems](https://epubs.siam.org/doi/pdf/10.1137/15M1025487) (2015)
- [FractalNet: Ultra-deep neural networks without residuals](https://arxiv.org/abs/1605.07648) (2016)
- [A proposal on machine learning via dynamical systems](https://link.springer.com/article/10.1007/s40304-017-0103-z) (2017)
- [Stable architectures for deep neural networks](https://iopscience.iop.org/article/10.1088/1361-6420/aa9a90/meta?casa_token=2bPH9NF1atgAAAAA:s1zabUy4XIbdKQ-2y-q6oJDHE2Zmq3ZdtNFC1cmYdXGfEtnrs1UnATuPRlpm8R1Vg3dmNxk) (2017)
- [The reversible residual network: Backpropagation without storing activations](https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html) (2017)
- [Mean field residual networks: On the edge of chaos](https://proceedings.neurips.cc/paper/2017/hash/81c650caac28cdefce4de5ddc18befa0-Abstract.html) (2017)
- [Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations](http://proceedings.mlr.press/v80/lu18d.html) (2018)
- [Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366) (2018)
- [Dynamical isometry and a mean field theory of RNNs: Gating enables signal propagation in recurrent neural networks](http://proceedings.mlr.press/v80/chen18i/chen18i.pdf) (2018)
- [ODE-inspired network design for single image super-resolution](https://openaccess.thecvf.com/content_CVPR_2019/html/He_ODE-Inspired_Network_Design_for_Single_Image_Super-Resolution_CVPR_2019_paper.html) (2019)
- [Invertible Residual Networks](https://proceedings.mlr.press/v97/behrmann19a.html) (2019)
- [Learning differential equations that are easy to solve](https://arxiv.org/pdf/2007.04504.pdf) (2020)
- [Score-based generative modeling through stochastic differential equations](https://arxiv.org/abs/2011.13456) (2020)
- [Continuous-in-Depth Neural Networks](https://arxiv.org/abs/2008.02389) (2020)
- [Optimizing neural networks via Koopman operator theory](https://proceedings.neurips.cc/paper/2020/hash/169806bb68ccbf5e6f96ddc60c40a044-Abstract.html) (2020)
- [Momentum Residual Neural Networks](http://proceedings.mlr.press/v139/sander21a/sander21a.pdf) (2021)
- [Learning strange attractors with reservoir systems](https://arxiv.org/abs/2108.05024) (2021)
- [On Neural Differential Equations](https://arxiv.org/abs/2202.02435) (2022)

### Using DS to Understand Modern ML:
- [Neural Tangent Kernel: Convergence and generalization in neural networks](https://proceedings.neurips.cc/paper/2018/hash/5a4be1fa34e62bb8a6ec6b91d2462f5a-Abstract.html) (2018)
- [Wide neural networks of any depth evolve as linear models under gradient descent](https://proceedings.neurips.cc/paper/2019/hash/0d1a9651497a38d8b1c3871c84528bd4-Abstract.html) (2019)
- [Implicit regularization of discrete gradient dynamics in linear neural networks](https://proceedings.neurips.cc/paper/2019/hash/f39ae9ff3a81f499230c4126e01f421b-Abstract.html) (2019)
- [Continuous-time models for stochastic optimization algorithms](https://proceedings.neurips.cc/paper/2019/hash/9cd78264cf2cd821ba651485c111a29a-Abstract.html) (2019)
- [Finite depth and width corrections to the Neural Tangent Kernel](https://arxiv.org/abs/1909.05989) (2019)
- [High-dimensional dynamics of generalization error in neural networks](https://www.sciencedirect.com/science/article/pii/S0893608020303117) (2020)
- [Stochasticity of deterministic gradient descent: Large learning rate for multiscale objective function](https://proceedings.neurips.cc//paper/2020/file/1b9a80606d74d3da6db2f1274557e644-Paper.pdf) (2020)
- [The heavy-tail phenomenon in SGD](http://proceedings.mlr.press/v139/gurbuzbalaban21a.html) (2021)
- [SGD in the large: Average-case analysis, asymptotics, and stepsize criticality](https://proceedings.mlr.press/v134/paquette21a.html) (2021)
- [Scaling properties of deep residual networks](https://arxiv.org/abs/2105.12245) (2021)
- [The future is log-Gaussian: ResNets and their infinite-depth-and-width limit at initialization](https://arxiv.org/pdf/2106.04013.pdf) (2021)
- [The high-dimensional asymptotics of first order methods with random data](https://arxiv.org/abs/2112.07572) (2021)
- [Phase diagram of SGD in high-dimensional two-layer neural networks](https://arxiv.org/abs/2202.00293) (2022)


### Using ML to Study DS:
- [PDE-Net: Learning PDEs from data](http://proceedings.mlr.press/v80/long18a.html?ref=https://githubhelp.com) (2017)
- [Universal Differential Equations for Scientific Machine Learning](https://arxiv.org/abs/2001.04385) (2020)
- [Bridging physics-based and data-driven modeling for learning dynamical systems](https://proceedings.mlr.press/v144/wang21a.html) (2021)
- [An end-to-end deep learning approach for extracting stochastic dynamical systems with Î±-stable Levy noise](https://arxiv.org/pdf/2201.13114.pdf) (2022)
- Physics-informed neural networks, data-driven discovery of complex systems using neural networks, solving/simulating differential equations using neural networks, integrating ML with physics-based modeling, etc. (too many to list here)


## Related Review Papers/Monographs/Textbooks/Lecture Notes
- [Dynamical Systems and Numerical Analysis](https://books.google.se/books?hl=en&lr=&id=ymoQA8s5pNIC&oi=fnd&pg=PR11&dq=dynamical+systems+and+numerical+analysis&ots=TYk2JZiNVG&sig=0mCqPchp17JHceSdTerWUMjjAhE&redir_esc=y#v=onepage&q=dynamical%20systems%20and%20numerical%20analysis&f=false) (1996)
- [The Elements of Statistical Learning](https://link.springer.com/book/10.1007/978-3-319-31089-3) (2009)
- [Understanding Machine Learning: From Theory to Algorithms](https://www.cs.huji.ac.il/w~shais/UnderstandingMachineLearning/) (2014)
- [Deep Learning](https://www.deeplearningbook.org/) (2016)
- [Brownian Motion, Martingales, and Stochastic Calculus](https://link.springer.com/book/10.1007/978-3-319-31089-3) (2016)
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/) (2019)
- [Machine learning and the physical sciences](https://journals.aps.org/rmp/abstract/10.1103/RevModPhys.91.045002) (2019)
- [Dive Into Deep Learning](https://d2l.ai/)
- [Deep Learning (NYU), Spring 2020](https://atcold.github.io/pytorch-Deep-Learning/)
- [Foundations of Deep Learning (Maryland), Fall 2020](http://www.cs.umd.edu/class/fall2020/cmsc828W/index.html)
- [Machine Learning, Dynamical Systems and Control](http://databookuw.com/)
- [Deep Learning Theory Review: An Optimal Control and Dynamical Systems Perspective](https://arxiv.org/abs/1908.10920) (2019)
- [Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don't](https://arxiv.org/abs/2009.10713) (2020)
- [Theoretical issues in deep networks](https://www.pnas.org/content/117/48/30039) (2020)
- [Patterns, predictions, and actions: A story about machine learning](https://mlstory.org/) (2021)
- [Dynamical Systems and Machine Learning](https://www.math.pku.edu.cn/amel/docs/20200719122925684287.pdf) (2021)
- [The Principles of Deep Learning Theory](https://arxiv.org/abs/2106.10165) (2021)
- [Deep learning: A statistical viewpoint](https://www.cambridge.org/core/journals/acta-numerica/article/deep-learning-a-statistical-viewpoint/7BCB89D860CEDDD5726088FAD64F2A5A) (2021)
- [Fit without fear: Remarkable mathematical phenomena of deep learning through the prism of interpolation](https://www.cambridge.org/core/journals/acta-numerica/article/fit-without-fear-remarkable-mathematical-phenomena-of-deep-learning-through-the-prism-of-interpolation/DBAC769EB7F4DBA5C4720932C2826014) (2021)
- [Rough Path Theory (ETH), Spring 2021](https://metaphor.ethz.ch/x/2021/fs/401-4611-21L/#recordings)
- [Nonlinear Dynamics (Georgia Tech), Spring 2022](https://chaosbook.org/course1/about.html) 


## Softwares/Libraries
- [PyTorch](https://pytorch.org/)
- [Jax](https://github.com/google/jax)
- [SciML Scientific Machine Learning Software](https://sciml.ai/roadmap/)


## Others (related articles, blogposts, tutorials, cool stuffs, etc.) 
- [Deep Learning: Our Miraculous Year 1990-1991](https://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html)
- [Software 2.0](https://karpathy.medium.com/software-2-0-a64152b37c35) (2017)
- [One model to learn them all](https://arxiv.org/abs/1706.05137) (2017)
- [Winner's curse? On pace, progress, and empirical rigor](https://openreview.net/forum?id=rJWF0Fywf) (2018)
- [The science of deep learning](https://www.pnas.org/content/117/48/30029) (2020)
- [The Dawning of a New Era in Applied Mathematics](https://www.ams.org/journals/notices/202104/rnoti-p565.pdf) (2021)
- [AI Summer](https://theaisummer.com/) 
- [Full Stack Deep Learning](https://fullstackdeeplearning.com/spring2021/)
- [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)
- [Sebastian Raschka's Resources](https://sebastianraschka.com/resources/)
- [Teach Yourself Computer Science](https://teachyourselfcs.com/)
- [Tesla AI](https://www.tesla.com/AI)

