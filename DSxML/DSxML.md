---
#layout: archive
title: "Topics at the Interface of Dynamical Systems and Modern ML"
permalink: /DSxML/
author_profile: true
redirect_from:
  - /DSxML
---

## Notebooks
TBW 
<br>


## Key Papers
#### Modern ML x Landmarks:
- [Learning internal representations by error-propagation](https://apps.dtic.mil/sti/pdfs/ADA164453.pdf) (1986)
- [Gradient-based learning applied to document recognition](https://ieeexplore.ieee.org/abstract/document/726791) (1998)
- [Learning Deep Architectures for AI](https://books.google.se/books?hl=en&lr=&id=cq5ewg7FniMC&oi=fnd&pg=PA1&dq=info:pYyS8T9g_kkJ:scholar.google.com&ots=Kpi8QTmpIy&sig=XfG1389bgdNINpRjGy67OReL9_c&redir_esc=y#v=onepage&q&f=false) (2009)
- [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a) (2010)
- [ImageNet classification with deep convolutional neural networks](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) (2012)
- [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199) (2013)
- [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572) (2014)
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) (2014)
- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) (2014)
- [Dropout: A Simple Way to Prevent Neural Networks from
Overfitting](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer,) (2014)
- [Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) (2016)
- [Generative Adversarial Nets](https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html) (2014)
- [Batch normalization: accelerating deep network training by reducing internal covariate shift](https://arxiv.org/abs/1502.03167) (2015)
- [Deep Learning](https://www.deeplearningbook.org/) (2016)
- [Attention is All You Need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) (2017)
- [On large-batch training for deep learning: generalization gap and sharp minima](https://arxiv.org/abs/1609.04836) (2017)
- [Understanding deep learning requires rethinking generalization](https://dl.acm.org/doi/abs/10.1145/3446776) (2017, 2021)


#### Modern ML x Dynamical Systems:
- [A proposal on machine learning via dynamical systems](https://link.springer.com/article/10.1007/s40304-017-0103-z) (2017)
- [Stable architectures for deep neural networks](https://iopscience.iop.org/article/10.1088/1361-6420/aa9a90/meta?casa_token=2bPH9NF1atgAAAAA:s1zabUy4XIbdKQ-2y-q6oJDHE2Zmq3ZdtNFC1cmYdXGfEtnrs1UnATuPRlpm8R1Vg3dmNxk) (2017)
- [The Reversible Residual Network: Backpropagation Without Storing Activations](https://proceedings.neurips.cc/paper/2017/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html) (2017)
- [Beyond finite layer neural networks: Bridging deep architectures and numerical differential equations](http://proceedings.mlr.press/v80/lu18d.html) (2018)
- [Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366) (2018)
- [ODE-inspired network design for single image super-resolution](https://openaccess.thecvf.com/content_CVPR_2019/html/He_ODE-Inspired_Network_Design_for_Single_Image_Super-Resolution_CVPR_2019_paper.html) (2019)
- [Invertible Residual Networks](https://proceedings.mlr.press/v97/behrmann19a.html) (2019)
- [Universal Differential Equations for Scientific Machine Learning](https://arxiv.org/abs/2001.04385) (2020)
- [Learning differential equations that are easy to solve](https://arxiv.org/pdf/2007.04504.pdf) (2020)
- [Momentum Residual Neural Networks](http://proceedings.mlr.press/v139/sander21a/sander21a.pdf) (2021)
- [On Neural Differential Equations](https://arxiv.org/abs/2202.02435) (2022)

## Other References
- [Software 2.0](https://karpathy.medium.com/software-2-0-a64152b37c35) (2017)
