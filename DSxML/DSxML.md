---
#layout: archive
title: "Topics at the Interface of Dynamical Systems and Modern ML"
permalink: /DSxML/
author_profile: true
redirect_from:
  - /DSxML
---

## Notebooks
TBW 
<br>


## Key Papers
#### Modern ML x Landmarks:
- [Learning internal representations by error-propagation](https://apps.dtic.mil/sti/pdfs/ADA164453.pdf) (1986)
- [Gradient-based learning applied to document recognition](https://ieeexplore.ieee.org/abstract/document/726791) (1998)
- [Learning Deep Architectures for AI](https://books.google.se/books?hl=en&lr=&id=cq5ewg7FniMC&oi=fnd&pg=PA1&dq=info:pYyS8T9g_kkJ:scholar.google.com&ots=Kpi8QTmpIy&sig=XfG1389bgdNINpRjGy67OReL9_c&redir_esc=y#v=onepage&q&f=false) (2009)
- [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a) (2010)
- [ImageNet classification with deep convolutional neural networks](https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) (2012)
- [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199) (2013)
- [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572) (2014)
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) (2014)
- [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) (2014)
- [Dropout: A Simple Way to Prevent Neural Networks from
Overfitting](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf?utm_content=buffer79b43&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer,) (2014)
- [Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html) (2016)
- [Generative Adversarial Nets](https://proceedings.neurips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html) (2014)
- [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) (2015)
- [Deep Learning](https://www.deeplearningbook.org/) (2016)
- [Attention is All You Need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html) (2017)
- [On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima](https://arxiv.org/abs/1609.04836) (2017)


#### Modern ML x Dynamical Systems:
- [Stable architectures for deep neural networks](https://iopscience.iop.org/article/10.1088/1361-6420/aa9a90/meta?casa_token=2bPH9NF1atgAAAAA:s1zabUy4XIbdKQ-2y-q6oJDHE2Zmq3ZdtNFC1cmYdXGfEtnrs1UnATuPRlpm8R1Vg3dmNxk) (2017)
- [Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366) (2018)


## Other References
- [Software 2.0](https://karpathy.medium.com/software-2-0-a64152b37c35) (2017)
